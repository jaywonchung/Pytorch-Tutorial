{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial\n",
    "\n",
    "### 3. Training and Evaluating a Model\n",
    "\n",
    "- General training setup\n",
    "- Managing and loading datasets\n",
    "- Experience the conventions of training and evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup torch and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General training setup\n",
    "\n",
    "We define the ```device``` variable, which specifies where we will run our training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU is the default device. If you are to run our model on cpu, the following content isn't relevent.\n",
    "\n",
    "However if we want to run our model on cuda, two thing must be satisfied:\n",
    "\n",
    "1. The input data (tensors) should be on cuda.  \n",
    "2. The model parameters should be on cuda.\n",
    "\n",
    "This can be achieved in two different ways:\n",
    "\n",
    "1. Creating tensors with argument device:\n",
    "    ```python\n",
    "    x = torch.randn((4,5), device=device)\n",
    "    ```\n",
    "2. Moving an object to ```device```:\n",
    "    ```python\n",
    "    model = SimpleNet().to(device)\n",
    "    ```\n",
    "    ```python\n",
    "    input_data = input_data.to(device)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Datasets\n",
    "\n",
    "```torchvision.datasets``` provides famous datasets, which are listed in the [documentation](https://pytorch.org/docs/stable/torchvision/datasets.html).\n",
    "\n",
    "Let us toy with the simplest dataset, MNIST. You can apply transforms to the data here with the ```transform``` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_train = torchvision.datasets.MNIST(root='MNIST', train=True, transform=transform, download=True)\n",
    "MNIST_test = torchvision.datasets.MNIST(root='MNIST', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoaders load batches from the dataset. Here we just used ```shuffle=True```, but it is also possible to have ```shuffle=False``` and use an instance from ```torch.utils.data.RandomSampler``` and pass it to the parameter ```sampler```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(MNIST_train, batch_size=64, shuffle=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(MNIST_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating custom datasets is also possible by definig a class that inherits ```torch.utils.data.Dataset```. You need to write functions ```__init__```, ```__getitem__```, and ```__len__```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, filename, division):\n",
    "        assert division in ['train', 'test']\n",
    "        with np.load(filename) as f:\n",
    "            self.x, self.y = f[f'x_{dtype}'], f[f'y_{dtype}']\n",
    "        \n",
    "    def __getitem__(self, ind):\n",
    "        return self.x[ind], self.y[ind]\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.x) == len(self.y)\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate\n",
    "\n",
    "Here we toy with the simplist network, LeNet, and use the MNIST dataset loaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if type(m) in [nn.Linear, nn.Conv2d]:\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                m.bias.data.fill_(0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the convention for the training logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "print_every = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set model to training mode. The opposite is ```.eval()```. This part is necessary if the model includes layers that behave differently on evaluation phase, like batch normalization or dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for batch_ind, (input_data, target_data) in enumerate(train_data_loader):\n",
    "        # Move data to device\n",
    "        input_data, target_data = input_data.to(device), target_data.to(device)\n",
    "\n",
    "        # Forward propagation\n",
    "        output = model(input_data)\n",
    "\n",
    "        # Calculate loss function\n",
    "        loss = F.cross_entropy(output, target_data)\n",
    "\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()    # This is equivalent to model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if batch_ind % print_every == 0:\n",
    "            train_log = f'Epoch {epoch+1:2d}/{epochs:2d}\\tLoss: {loss.cpu().item():.6f}\\tTrain: [{batch_ind+1}/{len(train_data_loader)} ({100.*batch_ind/len(train_data_loader):.0f}%)]            '\n",
    "            print(train_log, end='\\r')\n",
    "            sys.stdout.flush()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the convention for the testing logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(test_data_loader)) as pbar:\n",
    "        for batch_ind, (input_data, target_data) in enumerate(test_data_loader):\n",
    "            # Move data to device\n",
    "            input_data, target_data = input_data.to(device), target_data.to(device)\n",
    "            \n",
    "            # Inference\n",
    "            output = model(input_data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            \n",
    "            # Count number of correct predictions\n",
    "            correct += pred.eq(target_data.view_as(pred)).sum()\n",
    "            \n",
    "            # Progress bar update\n",
    "            pbar.update(1)\n",
    "\n",
    "print(f'Test accuracy: {100. * int(correct) / len(MNIST_test)}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
