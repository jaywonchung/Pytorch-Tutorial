{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial\n",
    "\n",
    "### 1. Tensors and Dynamic Graphs\n",
    "\n",
    "- Basic Operations involving ```torch.Tensor```.\n",
    "- Introduction to the dynamic graphs of ```torch.Autograd```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup torch and some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(4,5) * 2\n",
    "b = torch.ones(4,5) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor a:\n",
      "tensor([[2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.]])\n",
      "\n",
      "Tensor b:\n",
      "tensor([[3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "print('Tensor a:')\n",
    "print(a)\n",
    "print()\n",
    "print('Tensor b:')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important feature of pytorch is its dynamic computational graph, which automatically computes the gradient with respect to tensors.\n",
    "However not all tensors have their gradients calculated; only those with ```requires_grad = True``` do. Values of these tensors are cached during computation in the background.\n",
    "\n",
    "```requires_grad``` is ```False``` by default when you create a tensor. This can be provided as an optional argument:\n",
    "```python\n",
    "a = torch.ones((4,5), requires_grad=True)\n",
    "b = torch.ones((4,5), requires_grad=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(a.requires_grad)\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.requires_grad = True\n",
    "b.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "gradient_tensor = a + b\n",
    "print(gradient_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are indeed situations where you do not want pytorch to keep track of the operations you make. ```with torch.no_grad():``` is for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    no_gradient_tensor = a + b\n",
    "\n",
    "print(no_gradient_tensor.requires_grad)\n",
    "print(no_gradient_tensor.data == gradient_tensor.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform simple operations to ```a``` and ```b```. The resulting ```m``` is a scalar value, represented in pytorch as a tensor with one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5., grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "y = a + b\n",
    "m = torch.mean(y)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following implicitly calls ```m.backward(torch.Tensor([1]))```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.is_leaf=True\n",
      "Gradient w.r.t tensor a:\n",
      "tensor([[0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n",
      "        [0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n",
      "        [0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n",
      "        [0.0500, 0.0500, 0.0500, 0.0500, 0.0500]])\n",
      "\n",
      "b.is_leaf=True\n",
      "Gradient w.r.t tensor b:\n",
      "tensor([[0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n",
      "        [0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n",
      "        [0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n",
      "        [0.0500, 0.0500, 0.0500, 0.0500, 0.0500]])\n",
      "\n",
      "y.is_leaf=False\n",
      "Gradient w.r.t tensor y:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(f'a.is_leaf={a.is_leaf}')\n",
    "print('Gradient w.r.t tensor a:')\n",
    "print(a.grad)\n",
    "print()\n",
    "print(f'b.is_leaf={b.is_leaf}')\n",
    "print('Gradient w.r.t tensor b:')\n",
    "print(b.grad)\n",
    "print()\n",
    "print(f'y.is_leaf={y.is_leaf}')\n",
    "print('Gradient w.r.t tensor y:')\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New gradients generated by back propagation are added up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.is_leaf=True\n",
      "Gradient w.r.t tensor a:\n",
      "tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000]])\n",
      "\n",
      "b.is_leaf=True\n",
      "Gradient w.r.t tensor b:\n",
      "tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000]])\n"
     ]
    }
   ],
   "source": [
    "print(f'a.is_leaf={a.is_leaf}')\n",
    "print('Gradient w.r.t tensor a:')\n",
    "print(a.grad)\n",
    "print()\n",
    "print(f'b.is_leaf={b.is_leaf}')\n",
    "print('Gradient w.r.t tensor b:')\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad = None\n",
    "b.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best part of dynamic computational graphs is that the graph is recreated from scratch at every iteration. Thus you don't have to encode all possible paths before you launch the training - what you run is what you differentiate.\n",
    "\n",
    "[Autograd mechanics - Pytorch master documentation](https://pytorch.org/docs/stable/notes/autograd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = torch.sum(a)\n"
     ]
    }
   ],
   "source": [
    "# torch.randn samples numbers from a standard normal distribution.\n",
    "if torch.randn(1).item() > 0:\n",
    "    print('y = torch.sum(a)')\n",
    "    y = torch.sum(a)\n",
    "else:\n",
    "    print('y = torch.sum(b)')\n",
    "    y = torch.sum(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.is_leaf=True\n",
      "Gradient w.r.t tensor a:\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "b.is_leaf=True\n",
      "Gradient w.r.t tensor b:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(f'a.is_leaf={a.is_leaf}')\n",
    "print('Gradient w.r.t tensor a:')\n",
    "print(a.grad)\n",
    "print()\n",
    "print(f'b.is_leaf={b.is_leaf}')\n",
    "print('Gradient w.r.t tensor b:')\n",
    "print(b.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
